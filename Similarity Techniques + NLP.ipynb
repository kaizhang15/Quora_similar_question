{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7448604d-56cc-5eb6-bbd1-8905bf716a60",
    "_uuid": "f9790ca4971837ca3245a62b5efb9623373a9ed3"
   },
   "source": [
    "## Predict Duplicate using basic ML + NLP techniques\n",
    "\n",
    "I am trying to predict the duplicate sentences using vector similarity calculations and NLP technique in this module and its other forked versions.\n",
    "\n",
    "BOW + Cosine/Euclidean/Manhattan/Jaccard/Minskowiski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "e7d4aa51-afc3-ef96-11e6-f944b9bdda73",
    "_uuid": "5e021639cb58c26706424ed6702d5b128ab0fbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordEmbeddings\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "542c44bf-ec1d-ba34-7ab0-dae0bfc5e25f",
    "_uuid": "3a46bcacbd03683eeaf17c9e2abc36f9875b5a0b"
   },
   "source": [
    "## Reading train data, Cleaning\n",
    "\n",
    "*Reading Training Data ,\n",
    "Removing duplicates , \n",
    "Removing NULL values*\n",
    "\n",
    "Using pandas read_csv command to read data from train files.  And doing some basic cleanup on the data by removing any duplicates or null values that may be present in the data. \n",
    "\n",
    "Note: Reducing the size of the data set so that the Kernel memory does not run out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "acafea10-9564-f25a-6849-d976ee90a51a",
    "_uuid": "94ea32fb7a1878b928c2cec356cf174dcfab52dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of base training File =  (404290, 6)\n",
      "Shape of base training data after cleaning =  (404288, 6)\n",
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv(\"./input/train.csv\")\n",
    "#     df = pd.read_csv(\"../input/train.csv\", nrows=20000)\n",
    "    print (\"Shape of base training File = \", df.shape)\n",
    "    # Remove missing values and duplicates from training data\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
    "    return df\n",
    "\n",
    "df_train = read_data()\n",
    "# df_train, df_test = train_test_split(df, test_size = 0.02)\n",
    "print (df_train.head(2))\n",
    "# print (df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3e20ba9-2e98-37dd-3d95-a3ebf9d9c1fb",
    "_uuid": "5ba04d2b180013670e0cc6b48f53fc356eb7c5e4"
   },
   "source": [
    "As we can see from the above output the file contains six columns\n",
    "\n",
    "- **qid1** & **qid2**  - which contains the unique id assigned to the question\n",
    "- **question1** & **question2** - which contains the actual questions\n",
    "- **is_duplicate** - which contains information if the question1 and 2 are duplicate or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7ff92a6d-8a72-cde1-24bb-093fb60fb452",
    "_uuid": "cc326d8b7c7f46980af90c4cd85d6f69aedd0318"
   },
   "source": [
    "## EDA ##\n",
    "\n",
    "Some EDA on the data to get a look and feel about the data. Here we are trying to see the distribution of output data. Duplicate questions available etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "ad969c65-aac3-f40c-4650-b50ccc143fa6",
    "_uuid": "c85c0f13c2d049bda91e6016293da5d482a2b73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Count = 149263 , Non Duplicate Count = 255025\n",
      "Unique Questions = 537931\n",
      "Count of Quesitons appearing more than once = 111778\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "def eda(df):\n",
    "    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n",
    "           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n",
    "    \n",
    "    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n",
    "    \n",
    "    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n",
    "    \n",
    "    question_ids_counter = Counter(question_ids_combined)\n",
    "    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n",
    "    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n",
    "    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n",
    "    \n",
    "    \n",
    "eda(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "607dec70-9919-5f49-3a56-0cd23c9e9670",
    "_uuid": "a7d1c839820342c06b1a84f892e0ea9c7cf68ac3"
   },
   "source": [
    "## Train Dictionary ##\n",
    "\n",
    "First we will tokenize the sentences to extract words from the question. Lets also apply porter stemmer to break down words into their basic form. This should help us increase the accuracy of the system.\n",
    "\n",
    "Then we use gensims to train a dictionary of words available in the corpus. We are training the dictionary based on the Bag Of Words concept. Gensims dictionary will assign a id to each word which we can use later to convert documents into vectors. \n",
    "\n",
    "Also, filter extremes to remove words appearing less than 5times in the corpus or in more than 80% of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "9eb1f3bb-0ba9-bef3-487b-1ad5a440abf9",
    "_uuid": "6aacb7c19b22722306385391129d3b03115759ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangkai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/zhangkai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of words in the dictionary = 7781\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "words = re.compile(r\"\\w+\",re.I)\n",
    "stopword = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_questions(df):\n",
    "    question_1_tokenized = []\n",
    "    question_2_tokenized = []\n",
    "\n",
    "    for q in df.question1.tolist():\n",
    "        question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    for q in df.question2.tolist():\n",
    "        question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    df[\"Question_1_tok\"] = question_1_tokenized\n",
    "    df[\"Question_2_tok\"] = question_2_tokenized\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_dictionary(df):\n",
    "    \n",
    "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(questions_tokenized)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "df_train = tokenize_questions(df_train)\n",
    "dictionary = train_dictionary(df_train)\n",
    "print (\"No. of words in the dictionary = %s\" %len(dictionary.token2id))\n",
    "\n",
    "df_test = tokenize_questions(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42fe02fe-bf22-fadb-b3b9-812d82e7b60a",
    "_uuid": "0df6f8c58a35b2351e81eedb1c31ba068cc696a6"
   },
   "source": [
    "As we can see that the number of unique words in the dictionary after filtering are 4831. \n",
    "This would be the size of each of the vector in the question set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56bed0e8-b345-d2cd-3816-1748b47c4483",
    "_uuid": "76528a90f661a48043f4c7a78e6f95f736400158"
   },
   "source": [
    "## Create Vector##\n",
    "\n",
    "Here we are using the simple method of Bag Of Words Technique to convert sentences into vectors. There are two vector matrices thus created where each of the matrix is a sparse matrix to save  memory in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c6c05814-8314-8af4-cabc-b39c51a58293",
    "_uuid": "9d5278909b187d26f7b94184cb5d55f51273b5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 7781)\n",
      "(49000, 7781)\n"
     ]
    }
   ],
   "source": [
    "def get_vectors(df, dictionary):\n",
    "    \n",
    "    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n",
    "    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n",
    "    \n",
    "    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
    "    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
    "    \n",
    "    return question1_csc.transpose(),question2_csc.transpose()\n",
    "\n",
    "\n",
    "q1_csc, q2_csc = get_vectors(df_train, dictionary)\n",
    "\n",
    "print (q1_csc.shape)\n",
    "print (q2_csc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0d40f181-547d-8f31-5633-847bff79dde1",
    "_uuid": "342882be016abf97efcb5723d6d139cb920c8b9d"
   },
   "source": [
    "As we can see each of the matrix is of size \n",
    "404288 X 30114   = > (size of the training data) X (no of words in the dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "113ed18c-6634-8ba2-f693-2521cfba2ac7",
    "_uuid": "83f27fe6421c4d4ca567677ef22f50873957bcb3"
   },
   "source": [
    "## Define Similarity Calculation Fucntions##\n",
    "\n",
    "Here we have defined various Distance calculation functions for \n",
    "\n",
    " - Cosine Distance\n",
    " - Euclidean Distance\n",
    " - Manhattan Distance\n",
    " - Jaccard Distance\n",
    " - Minkowski Distance\n",
    "\n",
    "As Eucledian, Manhattan and Minkowski Distance may go beyond 1 we must scale them down between0 - 1 , for that we are using MinMaxScaler and training them on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "8b90d4fd-5c98-9f58-3e6e-1ed968b6f869",
    "_uuid": "a2313073a9c969bf4ab416baa175a18e5e268ebc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim sample= \n",
      " [0.59999999999999998, 0.48997894350611149]\n",
      "manhattan_dis sample = \n",
      " [4.0, 21.0]\n",
      "eucledian_dis sample = \n",
      " [2.0, 4.7958315233127191]\n",
      "jaccard_dis sample = \n",
      " [0.42857142857142855, 0]\n",
      "minkowsk_dis sample = \n",
      " [2.0, 4.7958315233127191]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics import jaccard_similarity_score as jsc\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
    "mms_scale_man = MinMaxScaler()\n",
    "mms_scale_euc = MinMaxScaler()\n",
    "mms_scale_mink = MinMaxScaler()\n",
    "\n",
    "def get_similarity_values(q1_csc, q2_csc):\n",
    "    cosine_sim = []\n",
    "    manhattan_dis = []\n",
    "    eucledian_dis = []\n",
    "    jaccard_dis = []\n",
    "    minkowsk_dis = []\n",
    "    \n",
    "    for i,j in zip(q1_csc, q2_csc):\n",
    "        sim = cs(i,j)\n",
    "        cosine_sim.append(sim[0][0])\n",
    "        sim = md(i,j)\n",
    "        manhattan_dis.append(sim[0][0])\n",
    "        sim = ed(i,j)\n",
    "        eucledian_dis.append(sim[0][0])\n",
    "        i_ = i.toarray()\n",
    "        j_ = j.toarray()\n",
    "        try:\n",
    "            sim = jsc(i_,j_)\n",
    "            jaccard_dis.append(sim)\n",
    "        except:\n",
    "            jaccard_dis.append(0)\n",
    "            \n",
    "        sim = minkowski_dis.pairwise(i_,j_)\n",
    "        minkowsk_dis.append(sim[0][0])\n",
    "    \n",
    "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n",
    "\n",
    "\n",
    "# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
    "cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n",
    "print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n",
    "print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n",
    "print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n",
    "print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n",
    "print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n",
    "\n",
    "eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n",
    "manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n",
    "minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n",
    "    \n",
    "manhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\n",
    "eucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\n",
    "minkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n",
    "\n",
    "eucledian_dis = eucledian_dis_array.flatten()\n",
    "manhattan_dis = manhattan_dis_array.flatten()\n",
    "minkowsk_dis = minkowsk_dis_array.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef529f38-e1c5-4c01-f250-65b37e8efc62",
    "_uuid": "c11e5494361a3fe3c4f2a2ac1b32f1be8a1c7f79"
   },
   "source": [
    "## Calculate Log Loss##\n",
    "\n",
    "Here we will use log loss formula to set a base criteria as to what accuracy our algorithm is able to achieve in terms of log loss which is the competition calucation score.\n",
    "\n",
    "We will also use Eucledian, Manhattan , Minkowski and Jaccard to calculate the similarity and then have a look at the log loss from each one of them. These are the five most widely used similarity classes used in Data Science so Lets use each one of them to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "8c4e9eb2-f52a-950e-8ddb-14110f64ba7b",
    "_uuid": "f9be0d7bacdb50ce67de0193c3929aa8a6235756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated log loss value on the test set for cosine sim is = 1.422442\n",
      "The calculated log loss value on the test set for manhattan sim is = 3.507409\n",
      "The calculated log loss value on the test set for euclidean sim is = 3.142903\n",
      "The calculated log loss value on the test set for jaccard sim is = 3.406602\n",
      "The calculated log loss value on the test set for minkowski sim is = 3.142903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def calculate_logloss(y_true, y_pred):\n",
    "    loss_cal = log_loss(y_true, y_pred)\n",
    "    return loss_cal\n",
    "\n",
    "q1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\n",
    "y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\n",
    "y_true = df_test.is_duplicate.tolist()\n",
    "\n",
    "y_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\n",
    "y_pred_man = y_pred_man_array.flatten()\n",
    "\n",
    "y_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\n",
    "y_pred_euc = y_pred_euc_array.flatten()\n",
    "\n",
    "y_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\n",
    "y_pred_mink = y_pred_mink_array.flatten()\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_cos)\n",
    "print (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_man)\n",
    "print (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_euc)\n",
    "print (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_jac)\n",
    "print (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_mink)\n",
    "print (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6715aaba-6649-6f89-f284-63765995f61f",
    "_uuid": "c8733da11e8f6bb0988d412591939feb3574f245"
   },
   "source": [
    "Although this test is run on a small set it indicates that cosine similarity is working as the best parameter for finding duplicate among sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82d19c90-b56f-06ff-2d82-68a28dc333b5",
    "_uuid": "3a7829ae8bee52fab08628a42d1eb3ef831a5724"
   },
   "source": [
    "## Adding Machine Learning Models to improve logloss accuracy ##\n",
    "\n",
    "Now in order to improve on the accuracy let us feed the results from these similarity coefficients to a Random Forest Regressor and Support Vector Regressor and check if we can improve on the log loss values.\n",
    "\n",
    "Not concentrating on the hyper parameters of RF and SVM we are just allowing the algorithms to run as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8022d9e6-5b04-9473-821c-7df290040fe4",
    "_uuid": "4c8adde766f5bc703644c268310f6aedd315027c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\n",
    "y_train = df_train.is_duplicate\n",
    "\n",
    "X_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\n",
    "y_test = y_true\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c1b70db8-92e5-f0f9-0374-49bcd4bc8a15",
    "_uuid": "7c6d3133c3d76f4c9a3cd8901bcd63f293238f52"
   },
   "source": [
    "Now that we have trained the model . Lets predict duplicate from models and calcualte logloss from them to check if their is any improvement in the logloss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "77cc19b8-a2a0-a3c3-2f63-df0496fdba52",
    "_uuid": "f51fa14daa558babaf099e46fa1bbf3d80682131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated log loss value on the test set using RFR is = 0.733248\n",
      "The calculated log loss value on the test set using SVR is = 0.660548\n"
     ]
    }
   ],
   "source": [
    "y_rfr_predicted = rfr.predict(X_test)\n",
    "y_svr_predicted = svr.predict(X_test)\n",
    "\n",
    "logloss_rfr = calculate_logloss(y_test, y_rfr_predicted)\n",
    "logloss_svr = calculate_logloss(y_test, y_svr_predicted)\n",
    "\n",
    "print (\"The calculated log loss value on the test set using RFR is = %f\" %logloss_rfr)\n",
    "print (\"The calculated log loss value on the test set using SVR is = %f\" %logloss_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "573a925a-ea42-132c-0b75-e51918d6828f",
    "_uuid": "3afe75a1ed4bc31fd0b6f13b32d7b9d2f412bd8a"
   },
   "source": [
    "As we can see from the above results that we are able to bring down the logloss values to nearly **half** of what was predicted earlier using base similarity techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5756cac-f00c-f9c8-ece2-141d9c292378",
    "_uuid": "f869074c569c4967d4ef75a270f20fa1fbdbeb4b"
   },
   "source": [
    "##Adding other features like word count ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
