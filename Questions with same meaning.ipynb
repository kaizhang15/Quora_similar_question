{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions with same meaning?\n",
    "This project uses the dataset containing many questions with label whether they are the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import sys\n",
    "from keras.layers.merge import add, concatenate\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data & primary processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All data files are in input dir.\n",
    "def Get_Data_Location(File_Dir, Trainset_name, Testset_name):\n",
    "    Train_File = File_Dir + Trainset_name\n",
    "    Test_File = File_Dir + Testset_name\n",
    "    return Train_File, Test_File\n",
    "\n",
    "def Read_Data(Train_File, Test_File):\n",
    "    data_train = pd.read_csv(Train_File, nrows=400000)\n",
    "    data_test = pd.read_csv(Test_File, nrows=4000)\n",
    "    data_train.drop_duplicates(inplace=True)\n",
    "    data_train.dropna(inplace=True)\n",
    "#     data_test.drop_duplicates(inplace=True)\n",
    "#     data_test.dropna(inplace=True)\n",
    "    print (\"Shape of train File = \", data_train.shape)\n",
    "    print (\"Shape of test File = \", data_test.shape)\n",
    "    print (data_train.head(2))\n",
    "    print (data_test.head(2))\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 16143 word vectors.\n"
     ]
    }
   ],
   "source": [
    "Vector_DIR = './input/'\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = codecs.open(os.path.join(Vector_DIR, 'vectors.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train File =  (399997, 6)\n",
      "Shape of test File =  (4000, 3)\n",
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "   test_id                                          question1  \\\n",
      "0        0  How does the Surface Pro himself 4 compare wit...   \n",
      "1        1  Should I have a hair transplant at age 24? How...   \n",
      "\n",
      "                                           question2  \n",
      "0  Why did Microsoft choose core m3 and not core ...  \n",
      "1        How much cost does hair transplant require?  \n"
     ]
    }
   ],
   "source": [
    "Train_File, Test_File = Get_Data_Location(\"./input/\", \"train.csv\", \"test.csv\")\n",
    "data_train, data_test = Read_Data(Train_File, Test_File)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the duplicated questions and get the statistic value of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def statistic_traindata(data_train):\n",
    "    Num_not_duplicate = data_train.is_duplicate.value_counts()[0]\n",
    "    Num_duplicate = data_train.is_duplicate.value_counts()[1]\n",
    "    print(\"Num_not_duplicate =\", Num_not_duplicate)\n",
    "    print(\"Num_duplicate =\", Num_duplicate)\n",
    "\n",
    "    total_question_list = data_train.qid1.tolist() + data_train.qid2.tolist()\n",
    "    total_unique_question = np.unique(total_question_list)\n",
    "    print(\"Total number of unique question =\", len(total_unique_question))\n",
    "\n",
    "    question_ids_counter = Counter(total_question_list)\n",
    "    More_than_once_question = []\n",
    "    for i in question_ids_counter.values():\n",
    "        if i > 1:\n",
    "            More_than_once_question.append(i)\n",
    "    print(\"Number of questions more than once =\", len(More_than_once_question))\n",
    "    return Num_not_duplicate, Num_duplicate, total_unique_question, More_than_once_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_not_duplicate = 252228\n",
      "Num_duplicate = 147769\n",
      "Total number of unique question = 533318\n",
      "Number of questions more than once = 110450\n"
     ]
    }
   ],
   "source": [
    "Num_not_duplicate, Num_duplicate, total_unique_question, More_than_once_question = statistic_traindata(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate dictionary\n",
    "Here, we tokenize the sentences to get words from the questions.\n",
    "Then, use porter stemmer to break down words into their basic form.\n",
    "Also use NLTK stopwords to ignore basic words and genism to train dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words in the dictionary = 21357\n"
     ]
    }
   ],
   "source": [
    "token_word = re.compile(r\"\\w+\",re.I)\n",
    "stopword = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_questions(df):\n",
    "    question_1_tokenized = []\n",
    "    question_2_tokenized = []\n",
    "\n",
    "    for q in df.question1.tolist():\n",
    "        question_1_tokenized.append([stemmer.stem(i.lower()) for i in token_word.findall(q) if i not in stopword])\n",
    "\n",
    "    for q in df.question2.tolist():\n",
    "        question_2_tokenized.append([stemmer.stem(i.lower()) for i in token_word.findall(q) if i not in stopword])\n",
    "\n",
    "    df[\"Question_1_tok\"] = question_1_tokenized\n",
    "    df[\"Question_2_tok\"] = question_2_tokenized\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_dictionary(df):\n",
    "    \n",
    "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(questions_tokenized)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "data_train = tokenize_questions(data_train)\n",
    "dictionary = train_dictionary(data_train)\n",
    "print (\"No. of words in the dictionary = %s\" %len(dictionary.token2id))\n",
    "\n",
    "data_test = tokenize_questions(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399997, 21357)\n",
      "(399997, 21357)\n",
      "(4000, 21357)\n",
      "(4000, 21357)\n"
     ]
    }
   ],
   "source": [
    "def get_vectors(data, dictionary):\n",
    "    \n",
    "    question1_vec = [dictionary.doc2bow(text) for text in data.Question_1_tok.tolist()]\n",
    "    question2_vec = [dictionary.doc2bow(text) for text in data.Question_2_tok.tolist()]\n",
    "    \n",
    "    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
    "    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
    "    \n",
    "    return question1_csc.transpose(),question2_csc.transpose()\n",
    "\n",
    "q1_csc, q2_csc = get_vectors(data_train, dictionary)\n",
    "test_q1_csc, test_q2_csc = get_vectors(data_test, dictionary)\n",
    "\n",
    "print (q1_csc.shape)\n",
    "print (q2_csc.shape)\n",
    "print (test_q2_csc.shape)\n",
    "print (test_q2_csc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim sample= \n",
      " [0.9486832980505138, 0.6154574548966638]\n",
      "test cosine_sim sample= \n",
      " [0.3698001308168194, 0.6454972243679029]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics import jaccard_similarity_score as jsc\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
    "mms_scale_man = MinMaxScaler()\n",
    "mms_scale_euc = MinMaxScaler()\n",
    "mms_scale_mink = MinMaxScaler()\n",
    "\n",
    "def get_similarity_values(q1_csc, q2_csc):\n",
    "    cosine_sim = []\n",
    "    manhattan_dis = []\n",
    "    eucledian_dis = []\n",
    "    jaccard_dis = []\n",
    "    minkowsk_dis = []\n",
    "    \n",
    "    for i,j in zip(q1_csc, q2_csc):\n",
    "        sim = cs(i,j)\n",
    "        cosine_sim.append(sim[0][0])\n",
    "#         sim = md(i,j)\n",
    "#         manhattan_dis.append(sim[0][0])\n",
    "#         sim = ed(i,j)\n",
    "#         eucledian_dis.append(sim[0][0])\n",
    "#         i_ = i.toarray()\n",
    "#         j_ = j.toarray()\n",
    "#         try:\n",
    "#             sim = jsc(i_,j_)\n",
    "#             jaccard_dis.append(sim)\n",
    "#         except:\n",
    "#             jaccard_dis.append(0)\n",
    "            \n",
    "#         sim = minkowski_dis.pairwise(i_,j_)\n",
    "#         minkowsk_dis.append(sim[0][0])\n",
    "    \n",
    "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n",
    "\n",
    "\n",
    "# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
    "cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n",
    "test_cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(test_q1_csc, test_q2_csc)\n",
    "\n",
    "print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n",
    "print (\"test cosine_sim sample= \\n\", test_cosine_sim[0:2])\n",
    "# print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n",
    "# print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n",
    "# print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n",
    "# print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n",
    "\n",
    "# eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n",
    "# manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n",
    "# minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n",
    "    \n",
    "# manhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\n",
    "# eucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\n",
    "# minkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n",
    "\n",
    "# eucledian_dis = eucledian_dis_array.flatten()\n",
    "# manhattan_dis = manhattan_dis_array.flatten()\n",
    "# minkowsk_dis = minkowsk_dis_array.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399997\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(cosine_sim))\n",
    "print(len(test_cosine_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = np.array([cosine_sim]).T\n",
    "test_dist = np.array([test_cosine_sim]).T\n",
    "# dist = [cosine_sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65659 unique tokens.\n",
      "Shape of data tensor: (399997, 30)\n",
      "Shape of label tensor: (399997,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_train.Question_1_tok.tolist() + data_train.Question_2_tok.tolist() + data_test.Question_1_tok.tolist() + data_test.Question_2_tok.tolist())\n",
    "sequences_1 = tokenizer.texts_to_sequences(data_train.Question_1_tok.tolist())\n",
    "sequences_2 = tokenizer.texts_to_sequences(data_train.Question_2_tok.tolist())\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(data_test.Question_1_tok.tolist())\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(data_test.Question_2_tok.tolist())\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(data_train.is_duplicate.tolist())\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_labels = np.array(data_test.test_id.tolist())\n",
    "# del test_sequences_1\n",
    "# del test_sequences_2\n",
    "# del sequences_1\n",
    "# del sequences_2\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index['NULL_Value'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Null word embeddings: 49518\n",
      "(65660, 50)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "# nb_words = len(word_index)\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_conv1D_(emb_matrix):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=60,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "    # Add the magic features\n",
    "#     magic_input = Input(shape=(5,))\n",
    "#     magic_dense = BatchNormalization()(magic_input)\n",
    "#     magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "    distance_input = Input(shape=(1,))\n",
    "    distance_dense = BatchNormalization()(distance_input)\n",
    "    distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "#     merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    merge = concatenate([diff, mul, distance_dense])\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "#     model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model = Model(inputs=[seq1, seq2, distance_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399997, 30)\n",
      "(399997, 1)\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_1.shape)\n",
    "print(dist.shape)\n",
    "print(test_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 319997 samples, validate on 80000 samples\n",
      "Epoch 1/1\n",
      "319997/319997 [==============================] - 741s 2ms/step - loss: 0.4676 - acc: 0.7661 - val_loss: 0.4121 - val_acc: 0.8003\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_48 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_49 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 60, 50)       3283000     input_48[0][0]                   \n",
      "                                                                 input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 60, 128)      6528        embedding_18[0][0]               \n",
      "                                                                 embedding_18[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 60, 128)      12928       embedding_18[0][0]               \n",
      "                                                                 embedding_18[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 60, 128)      19328       embedding_18[0][0]               \n",
      "                                                                 embedding_18[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 60, 128)      25728       embedding_18[0][0]               \n",
      "                                                                 embedding_18[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 60, 32)       8032        embedding_18[0][0]               \n",
      "                                                                 embedding_18[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 60, 32)       9632        embedding_18[0][0]               \n",
      "                                                                 embedding_18[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_175 (G (None, 128)          0           conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_177 (G (None, 128)          0           conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_179 (G (None, 128)          0           conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_181 (G (None, 128)          0           conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_183 (G (None, 32)           0           conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_185 (G (None, 32)           0           conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_176 (G (None, 128)          0           conv1d_97[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_178 (G (None, 128)          0           conv1d_98[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_180 (G (None, 128)          0           conv1d_99[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_182 (G (None, 128)          0           conv1d_100[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_184 (G (None, 32)           0           conv1d_101[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_186 (G (None, 32)           0           conv1d_102[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 576)          0           global_average_pooling1d_175[0][0\n",
      "                                                                 global_average_pooling1d_177[0][0\n",
      "                                                                 global_average_pooling1d_179[0][0\n",
      "                                                                 global_average_pooling1d_181[0][0\n",
      "                                                                 global_average_pooling1d_183[0][0\n",
      "                                                                 global_average_pooling1d_185[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 576)          0           global_average_pooling1d_176[0][0\n",
      "                                                                 global_average_pooling1d_178[0][0\n",
      "                                                                 global_average_pooling1d_180[0][0\n",
      "                                                                 global_average_pooling1d_182[0][0\n",
      "                                                                 global_average_pooling1d_184[0][0\n",
      "                                                                 global_average_pooling1d_186[0][0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 1)            4           input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 576)          0           concatenate_48[0][0]             \n",
      "                                                                 concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 576)          0           concatenate_48[0][0]             \n",
      "                                                                 concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 128)          256         batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 1280)         0           lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 1280)         0           concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 1280)         5120        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 300)          384300      batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 300)          0           dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 300)          1200        dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 1)            301         batch_normalization_45[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,756,357\n",
      "Trainable params: 470,195\n",
      "Non-trainable params: 3,286,162\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "model = model_conv1D_(embedding_matrix)\n",
    "model.fit([data_1,data_2,dist], labels, validation_split=VALIDATION_SPLIT, epochs=1, batch_size=1024, shuffle=True)\n",
    "preds = model.predict([test_data_1, test_data_2, test_dist])\n",
    "print(model.summary())\n",
    "print(preds.shape)\n",
    "\n",
    "out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "out_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import add, concatenate\n",
    "# Model Architecture #\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = Conv1D(128, 3, activation='relu')(embedded_sequences_1)\n",
    "x1 = MaxPooling1D(10)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(64, activation='relu')(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = Conv1D(128, 3, activation='relu')(embedded_sequences_2)\n",
    "y1 = MaxPooling1D(10)(y1)\n",
    "y1 = Flatten()(y1)\n",
    "y1 = Dense(64, activation='relu')(y1)\n",
    "y1 = Dropout(0.2)(y1)\n",
    "\n",
    "# merged = merge([x1,y1], mode='concat')\n",
    "merged = concatenate([x1,y1])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(inputs=[sequence_1_input,sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 319997 samples, validate on 80000 samples\n",
      "Epoch 1/1\n",
      "319997/319997 [==============================] - 136s 425us/step - loss: 0.6376 - acc: 0.6168 - val_loss: 0.6072 - val_acc: 0.6484\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_78 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_79 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 30, 50)       3283000     input_78[0][0]                   \n",
      "                                                                 input_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 28, 128)      19328       embedding_2[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 28, 128)      19328       embedding_2[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 2, 128)       0           conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 2, 128)       0           conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 256)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 256)          0           max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 64)           16448       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 64)           16448       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 64)           0           dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 64)           0           dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 128)          0           dropout_42[0][0]                 \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 128)          512         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 64)           8256        batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 64)           0           dense_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 64)           256         dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 1)            65          batch_normalization_63[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,363,641\n",
      "Trainable params: 80,257\n",
      "Non-trainable params: 3,283,384\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "model.fit([data_1,data_2], labels, validation_split=VALIDATION_SPLIT, epochs=1, batch_size=1024, shuffle=True)\n",
    "preds = model.predict([test_data_1, test_data_2])\n",
    "print(model.summary())\n",
    "print(preds.shape)\n",
    "\n",
    "out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "out_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"vectors_test.txt\",\"w\") as f:\n",
    "    for i in data_train.Question_1_tok.tolist():\n",
    "        for j in i:\n",
    "            f.write(j+\" \")\n",
    "    for i in data_train.Question_2_tok.tolist():\n",
    "        for j in i:\n",
    "            f.write(j+\" \")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_rnn1D_(emb_matrix):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=60,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    rnn = LSTM(256,dropout=0.2,recurrent_dropout = 0.1)\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    rnn1a = rnn(emb1)\n",
    "    rnn1b = rnn(emb2)\n",
    "    \n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(256,))([rnn1a, rnn1b])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(256,))([rnn1a, rnn1b])\n",
    "\n",
    "    # Add the magic features\n",
    "#     magic_input = Input(shape=(5,))\n",
    "#     magic_dense = BatchNormalization()(magic_input)\n",
    "#     magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "    distance_input = Input(shape=(1,))\n",
    "    distance_dense = BatchNormalization()(distance_input)\n",
    "    distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "#     merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    merge = concatenate([diff, mul, distance_dense])\n",
    "    # The MLP that determines the outcome\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(merge)\n",
    "\n",
    "#     model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model = Model(inputs=[seq1, seq2, distance_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 319997 samples, validate on 80000 samples\n",
      "Epoch 1/1\n",
      "319997/319997 [==============================] - 1320s 4ms/step - loss: 0.4971 - acc: 0.7375 - val_loss: 0.4530 - val_acc: 0.7711\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_64 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_65 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 60, 50)       3283000     input_64[0][0]                   \n",
      "                                                                 input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_66 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 256)          314368      embedding_24[0][0]               \n",
      "                                                                 embedding_24[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 1)            4           input_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 256)          0           lstm_3[0][0]                     \n",
      "                                                                 lstm_3[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 256)          0           lstm_3[0][0]                     \n",
      "                                                                 lstm_3[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 128)          256         batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 640)          0           lambda_39[0][0]                  \n",
      "                                                                 lambda_40[0][0]                  \n",
      "                                                                 dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 1)            641         concatenate_59[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 3,598,269\n",
      "Trainable params: 315,267\n",
      "Non-trainable params: 3,283,002\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "model = model_rnn1D_(embedding_matrix)\n",
    "model.fit([data_1,data_2,dist], labels, validation_split=VALIDATION_SPLIT, epochs=1, batch_size=1024, shuffle=True)\n",
    "preds = model.predict([test_data_1, test_data_2, test_dist])\n",
    "print(model.summary())\n",
    "print(preds.shape)\n",
    "\n",
    "out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "out_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D\n",
    "def model_mix1D_(emb_matrix):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=60,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "    rnn = LSTM(256,dropout=0.2,recurrent_dropout = 0.1)\n",
    "    \n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalMaxPooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalMaxPooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalMaxPooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalMaxPooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalMaxPooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalMaxPooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalMaxPooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalMaxPooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalMaxPooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalMaxPooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalMaxPooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalMaxPooling1D()(conv6b)\n",
    "    \n",
    "    rnn1a = rnn(emb1)\n",
    "    rnn1b = rnn(emb2)\n",
    "    \n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a,rnn1a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b,rnn1b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32 +256,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32 + 256,))([mergea, mergeb])\n",
    "\n",
    "    # Add the magic features\n",
    "#     magic_input = Input(shape=(5,))\n",
    "#     magic_dense = BatchNormalization()(magic_input)\n",
    "#     magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "    distance_input = Input(shape=(1,))\n",
    "    distance_dense = BatchNormalization()(distance_input)\n",
    "    distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "#     merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    merge = concatenate([diff, mul, distance_dense])\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "#     model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model = Model(inputs=[seq1, seq2, distance_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 319997 samples, validate on 80000 samples\n",
      "Epoch 1/1\n",
      "319997/319997 [==============================] - 2443s 8ms/step - loss: 0.4558 - acc: 0.7777 - val_loss: 0.3905 - val_acc: 0.8112\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_67 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_68 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 60, 50)       3283000     input_67[0][0]                   \n",
      "                                                                 input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, 60, 128)      6528        embedding_25[0][0]               \n",
      "                                                                 embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, 60, 128)      12928       embedding_25[0][0]               \n",
      "                                                                 embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, 60, 128)      19328       embedding_25[0][0]               \n",
      "                                                                 embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, 60, 128)      25728       embedding_25[0][0]               \n",
      "                                                                 embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 60, 32)       8032        embedding_25[0][0]               \n",
      "                                                                 embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 60, 32)       9632        embedding_25[0][0]               \n",
      "                                                                 embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 32)           0           conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 32)           0           conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 256)          314368      embedding_25[0][0]               \n",
      "                                                                 embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           conv1d_121[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           conv1d_122[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           conv1d_123[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           conv1d_124[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 32)           0           conv1d_125[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 32)           0           conv1d_126[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_69 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 832)          0           global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 832)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_24[0][0]    \n",
      "                                                                 lstm_4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 1)            4           input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 832)          0           concatenate_60[0][0]             \n",
      "                                                                 concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 832)          0           concatenate_60[0][0]             \n",
      "                                                                 concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 128)          256         batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 1792)         0           lambda_41[0][0]                  \n",
      "                                                                 lambda_42[0][0]                  \n",
      "                                                                 dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 1792)         0           concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 1792)         7168        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 300)          537900      batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 300)          0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 300)          1200        dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 1)            301         batch_normalization_56[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 4,226,373\n",
      "Trainable params: 939,187\n",
      "Non-trainable params: 3,287,186\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "model = model_mix1D_(embedding_matrix)\n",
    "model.fit([data_1,data_2,dist], labels, validation_split=VALIDATION_SPLIT, epochs=1, batch_size=1024, shuffle=True)\n",
    "preds = model.predict([test_data_1, test_data_2, test_dist])\n",
    "print(model.summary())\n",
    "print(preds.shape)\n",
    "\n",
    "out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "out_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dis = dis.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis.shape\n",
    "type(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
