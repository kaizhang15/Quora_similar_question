{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions with same meaning?\n",
    "This project uses the dataset containing many questions with label whether they are the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import sys\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data & primary processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All data files are in input dir.\n",
    "def Get_Data_Location(File_Dir, Trainset_name, Testset_name):\n",
    "    Train_File = File_Dir + Trainset_name\n",
    "    Test_File = File_Dir + Testset_name\n",
    "    return Train_File, Test_File\n",
    "\n",
    "def Read_Data(Train_File, Test_File):\n",
    "    data_train = pd.read_csv(Train_File, nrows = 20000)\n",
    "    data_test = pd.read_csv(Test_File, nrows=20000)\n",
    "    data_train.drop_duplicates(inplace=True)\n",
    "    data_train.dropna(inplace=True)\n",
    "    data_test.drop_duplicates(inplace=True)\n",
    "    data_test.dropna(inplace=True)\n",
    "    print (\"Shape of train File = \", data_train.shape)\n",
    "    print (\"Shape of test File = \", data_test.shape)\n",
    "    print (data_train.head(2))\n",
    "    print (data_test.head(2))\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 16143 word vectors.\n"
     ]
    }
   ],
   "source": [
    "Vector_DIR = './input/'\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = codecs.open(os.path.join(Vector_DIR, 'vectors.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train File =  (20000, 6)\n",
      "Shape of test File =  (20000, 3)\n",
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "   test_id                                          question1  \\\n",
      "0        0  How does the Surface Pro himself 4 compare wit...   \n",
      "1        1  Should I have a hair transplant at age 24? How...   \n",
      "\n",
      "                                           question2  \n",
      "0  Why did Microsoft choose core m3 and not core ...  \n",
      "1        How much cost does hair transplant require?  \n"
     ]
    }
   ],
   "source": [
    "Train_File, Test_File = Get_Data_Location(\"./input/\", \"train.csv\", \"test.csv\")\n",
    "data_train, data_test = Read_Data(Train_File, Test_File)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the duplicated questions and get the statistic value of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def statistic_traindata(data_train):\n",
    "    Num_not_duplicate = data_train.is_duplicate.value_counts()[0]\n",
    "    Num_duplicate = data_train.is_duplicate.value_counts()[1]\n",
    "    print(\"Num_not_duplicate =\", Num_not_duplicate)\n",
    "    print(\"Num_duplicate =\", Num_duplicate)\n",
    "\n",
    "    total_question_list = data_train.qid1.tolist() + data_train.qid2.tolist()\n",
    "    total_unique_question = np.unique(total_question_list)\n",
    "    print(\"Total number of unique question =\", len(total_unique_question))\n",
    "\n",
    "    question_ids_counter = Counter(total_question_list)\n",
    "    More_than_once_question = []\n",
    "    for i in question_ids_counter.values():\n",
    "        if i > 1:\n",
    "            More_than_once_question.append(i)\n",
    "    print(\"Number of questions more than once =\", len(More_than_once_question))\n",
    "    return Num_not_duplicate, Num_duplicate, total_unique_question, More_than_once_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_not_duplicate = 12526\n",
      "Num_duplicate = 7474\n",
      "Total number of unique question = 37767\n",
      "Number of questions more than once = 1806\n"
     ]
    }
   ],
   "source": [
    "Num_not_duplicate, Num_duplicate, total_unique_question, More_than_once_question = statistic_traindata(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate dictionary\n",
    "Here, we tokenize the sentences to get words from the questions.\n",
    "Then, use porter stemmer to break down words into their basic form.\n",
    "Also use NLTK stopwords to ignore basic words and genism to train dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words in the dictionary = 4890\n"
     ]
    }
   ],
   "source": [
    "token_word = re.compile(r\"\\w+\",re.I)\n",
    "stopword = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_questions(df):\n",
    "    question_1_tokenized = []\n",
    "    question_2_tokenized = []\n",
    "\n",
    "    for q in df.question1.tolist():\n",
    "        question_1_tokenized.append([stemmer.stem(i.lower()) for i in token_word.findall(q) if i not in stopword])\n",
    "\n",
    "    for q in df.question2.tolist():\n",
    "        question_2_tokenized.append([stemmer.stem(i.lower()) for i in token_word.findall(q) if i not in stopword])\n",
    "\n",
    "    df[\"Question_1_tok\"] = question_1_tokenized\n",
    "    df[\"Question_2_tok\"] = question_2_tokenized\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_dictionary(df):\n",
    "    \n",
    "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(questions_tokenized)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "data_train = tokenize_questions(data_train)\n",
    "dictionary = train_dictionary(data_train)\n",
    "print (\"No. of words in the dictionary = %s\" %len(dictionary.token2id))\n",
    "\n",
    "data_test = tokenize_questions(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22985 unique tokens.\n",
      "Shape of data tensor: (20000, 30)\n",
      "Shape of label tensor: (20000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_train.Question_1_tok.tolist() + data_train.Question_2_tok.tolist() + data_test.Question_1_tok.tolist() + data_test.Question_2_tok.tolist())\n",
    "sequences_1 = tokenizer.texts_to_sequences(data_train.Question_1_tok.tolist())\n",
    "sequences_2 = tokenizer.texts_to_sequences(data_train.Question_2_tok.tolist())\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(data_test.Question_1_tok.tolist())\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(data_test.Question_2_tok.tolist())\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(data_train.is_duplicate.tolist())\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_labels = np.array(data_test.test_id.tolist())\n",
    "# del test_sequences_1\n",
    "# del test_sequences_2\n",
    "# del sequences_1\n",
    "# del sequences_2\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['NULL_Value'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Null word embeddings: 9487\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "# nb_words = len(word_index)\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_conv1D_(emb_matrix):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=60,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "    # Add the magic features\n",
    "#     magic_input = Input(shape=(5,))\n",
    "#     magic_dense = BatchNormalization()(magic_input)\n",
    "#     magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "#     distance_input = Input(shape=(20,))\n",
    "#     distance_dense = BatchNormalization()(distance_input)\n",
    "#     distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "#     merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    merge = concatenate([diff, mul])\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "#     model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model = Model(inputs=[seq1, seq2], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/1\n",
      "16000/16000 [==============================] - 24s 1ms/step - loss: 0.6909 - acc: 0.6517 - val_loss: 0.5490 - val_acc: 0.7305\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "model = model_conv1D_(embedding_matrix)\n",
    "model.fit([data_1,data_2], labels, validation_split=VALIDATION_SPLIT, epochs=1, batch_size=1024, shuffle=True)\n",
    "preds = model.predict([test_data_1, test_data_2])\n",
    "print(preds.shape)\n",
    "\n",
    "out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "out_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import add, concatenate\n",
    "# Model Architecture #\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = Conv1D(128, 3, activation='relu')(embedded_sequences_1)\n",
    "x1 = MaxPooling1D(10)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(64, activation='relu')(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = Conv1D(128, 3, activation='relu')(embedded_sequences_2)\n",
    "y1 = MaxPooling1D(10)(y1)\n",
    "y1 = Flatten()(y1)\n",
    "y1 = Dense(64, activation='relu')(y1)\n",
    "y1 = Dropout(0.2)(y1)\n",
    "\n",
    "# merged = merge([x1,y1], mode='concat')\n",
    "merged = concatenate([x1,y1])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(inputs=[sequence_1_input,sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/1\n",
      "16000/16000 [==============================] - 5s 302us/step - loss: 0.7486 - acc: 0.5183 - val_loss: 0.6751 - val_acc: 0.5205\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "model.fit([data_1,data_2], labels, validation_split=VALIDATION_SPLIT, epochs=1, batch_size=1024, shuffle=True)\n",
    "preds = model.predict([test_data_1, test_data_2])\n",
    "print(preds.shape)\n",
    "\n",
    "out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "out_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"vectors_test.txt\",\"w\") as f:\n",
    "    for i in data_train.Question_1_tok.tolist():\n",
    "        for j in i:\n",
    "            f.write(j+\" \")\n",
    "    for i in data_train.Question_2_tok.tolist():\n",
    "        for j in i:\n",
    "            f.write(j+\" \")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
